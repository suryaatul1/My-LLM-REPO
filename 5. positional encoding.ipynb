{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81542f62-5a0a-4153-8a54-7fd938541e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset , DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d9d48-ff7c-489e-a480-cf025707ea77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Embedding:</b> \n",
    "<p>    \n",
    "We are working on creating the POSITIONAL Embedding. This is required to give the token\n",
    "embedding additional information about the posistion of text , so model does not overfit for \n",
    "repeated phrases\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc494b-742f-4f20-be4b-55d3db69954b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Token Encoding:</b> \n",
    "<p>    \n",
    "Below we use\n",
    "\n",
    "    1. BPE (Byte Pair Encoding mechanism for tokenization of text\n",
    "    2. Then we create input - Target pair using pytorch dataset\n",
    "    \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ee2031b-de95-404c-89ad-f9fd7cbcddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text , tokenizer , max_length , stride):\n",
    "        self.input_tensor=[]\n",
    "        self.output_tensor=[]\n",
    "\n",
    "        # Tokenize entire text usng gpt2 BPE\n",
    "        token_ids=tokenizer.encode(text , allowed_special={\"<|endoftext|>\"})\n",
    "        print(\"Length of Token Ids\",len(token_ids))\n",
    "        \n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            output_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_tensor.append(torch.tensor(input_chunk))\n",
    "            self.output_tensor.append(torch.tensor(output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(\"get next item\")\n",
    "        return self.input_tensor[idx] , self.output_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69027207-b99b-4dcc-a81e-736511008b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(text , batch_size=4,max_length=256,\n",
    "                           stride=128,shuffle=True,drop_last=True,\n",
    "                            num_workers=0):\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset=GPTDatasetV1(text,tokenizer,max_length,stride)\n",
    "\n",
    "    dataloader=DataLoader(\n",
    "                          dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          drop_last=drop_last,\n",
    "                          num_workers=num_workers\n",
    "                         )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a970239-d76e-4f3c-b6c4-bea792fa3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/the-verdict.txt\" ,'r',encoding=\"utf-8\" ) as f:\n",
    "    raw_text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc7218ce-6c12-49fc-87a9-275a0023924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Token Ids 5145\n"
     ]
    }
   ],
   "source": [
    "max_length=4\n",
    "dataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=max_length,\n",
    "                     stride=max_length,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f299a4c-54c1-4860-b5fb-f057e17fb4c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>End of creating the datasets</b> \n",
    "<p>    \n",
    "This is the end we load the dataset and create the datasets as:\n",
    "\n",
    "    1. Input - Target Pair\n",
    "    2. Create data in  batches\n",
    "    3. Create the data with context window defined , in this case it = max_length\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b2085-2b42-4cf7-bcdf-f7ef6d64e205",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0305f99-99f1-4c13-be8a-8a324873582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=50257  #BPE gpt2 has fixed vocab length of 50257\n",
    "output_dim=256    # The tokens will further be expaned vector space of 256 dimensions\n",
    "\n",
    "token_embedding_layer=torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e083100-5046-4772-8b6b-7adc8dfb240c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the inputs: torch.Size([8, 4])\n",
      "Shape of the token embedded: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# here we are actually taking just the 1st Batch and then performing\n",
    "# Token encoding\n",
    "# check the size of input and token encoding , it is just for 1 batch\n",
    "# becuase we defined context window of 4 (mmax_length) and batch_size=8\n",
    "\n",
    "data_iter=iter(dataloader)\n",
    "inputs, output = next(data_iter)\n",
    "token_embeddings=token_embedding_layer(inputs)\n",
    "\n",
    "print(\"Shape of the inputs:\",inputs.shape)\n",
    "print(\"Shape of the token embedded:\",token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39221f68-0cb0-4763-9554-1973280332e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Token Embedding for all of the tokens and batches</b> \n",
    "<p>    \n",
    "We saw the previous cell. It only takes 1 batch of tokens.\n",
    "The Below example is for all of the tokens from all batches\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0d07782-c322-4ba1-bd2e-01a3419bebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings shape: torch.Size([1280, 4, 256])\n",
      "Number of token ID'd 1280\n",
      "Number of Batches created 160\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "tot=0\n",
    "batches=0\n",
    "\n",
    "# We loop through dataloader to get all batches and\n",
    "# encode them collect into list , at the end we concate the items of list\n",
    "\n",
    "for batch in dataloader:\n",
    "    token_ids1 = batch[0]                           # shape (batch_size,)\n",
    "    tot = len(token_ids1) + tot\n",
    "    emb = token_embedding_layer(token_ids1)          # shape (batch_size, embed_dim)\n",
    "    all_embeddings.append(emb)\n",
    "    batches += 1\n",
    "# Concatenate all batches back together\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)x\n",
    "\n",
    "print(\"All embeddings shape:\", all_embeddings.shape)\n",
    "print(\"Number of token ID'd\", tot)\n",
    "print(\"Number of Batches created\", batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4815e41-0b86-46bb-af8f-772abb6f28e8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad249d7-1f4d-499d-81b8-46e6f74bfd4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Creating Positional embedding</b> \n",
    "<p>    \n",
    "From here On  , we will work on creatig the Positional embedding\n",
    "\n",
    "The dimension of this positional encoding as to be = (context_length * dimensional encoding)\n",
    "which will be equal to (4 * 256) . Check the final token embedding it is (1280 * 4 * 256)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84a7ae7e-cf8b-4193-9021-0f8df88d2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=max_length\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2eb3fc-e55b-4556-a37b-88ea1fe8be98",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3309a0-c8da-41ba-b8f3-ceec17aaac5b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<p>    \n",
    "Lets look at some torch and embedding understanding\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f48238b7-d0ee-491c-a8db-afdc67c19892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pos_embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3da9bdeb-dc59-4d32-b67f-60d6f35514ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-D matrix :- tensor([0, 1, 2, 3])\n",
      "Size of the embedding now:- torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Lets look at some basics\n",
    "# torch.arange , creates a 1-D matrix\n",
    "print(\"1-D matrix :-\", torch.arange(context_length))\n",
    "\n",
    "# Now these pos_embedding is a look up matrix .\n",
    "# meaning if we pass the tensor it will map the value of tensor to the index and gives those rWindowsError\n",
    "\n",
    "out=pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Size of the embedding now:-\",out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96d5afea-1562-47ff-bf26-657040b8ee47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9574e-01,  2.0297e+00, -1.9559e-01,  9.8399e-01,  1.0991e+00,\n",
       "         -6.4257e-02,  5.6272e-01,  4.7487e-01, -7.2253e-01, -2.3067e-01,\n",
       "         -1.3186e-01,  9.8797e-01, -2.0638e+00, -7.9027e-01, -2.0503e-01,\n",
       "         -3.2141e-01,  1.1196e+00, -2.9789e-01,  8.7125e-02, -4.4571e-01,\n",
       "          9.9954e-01, -8.3010e-02, -4.3924e-01, -3.5169e-01,  1.3903e-01,\n",
       "          1.3182e-01,  6.8044e-01,  4.1842e-01,  8.4340e-02,  7.1125e-01,\n",
       "         -1.0704e+00,  7.7222e-02,  1.5551e-01, -8.2345e-01,  1.8396e+00,\n",
       "          4.0773e-01,  1.1029e+00,  1.2878e+00,  8.9099e-01, -1.8520e+00,\n",
       "         -3.3168e-01,  7.5198e-01, -2.7836e+00, -7.5571e-01, -1.5996e+00,\n",
       "          1.4584e+00,  5.9436e-01, -1.1413e+00,  6.0807e-01, -2.9501e-01,\n",
       "         -3.9207e-01, -5.7350e-01,  2.0634e+00, -2.6630e-01,  1.7259e-01,\n",
       "          5.3171e-01, -6.5363e-01, -2.2981e-01, -7.0085e-01,  8.8571e-02,\n",
       "         -1.5989e+00, -1.4588e+00,  1.4049e+00,  4.7040e-02,  6.9556e-01,\n",
       "         -8.5004e-01,  6.4517e-02,  1.4657e+00,  1.4896e+00,  1.0317e+00,\n",
       "         -5.2508e-01,  1.4417e-01, -5.3348e-01, -6.7799e-01,  8.9793e-01,\n",
       "          9.9800e-01, -7.6053e-01, -1.9534e-01,  2.8492e-01, -6.7734e-02,\n",
       "          6.8484e-01, -5.2655e-01, -5.2342e-01,  8.6408e-02,  3.2703e-01,\n",
       "         -3.2638e-01,  5.9028e-01, -1.0680e+00, -3.5608e-01, -1.0811e+00,\n",
       "          9.2034e-02, -6.3273e-02, -2.3500e-01,  5.4376e-02,  1.1568e+00,\n",
       "          1.6684e+00,  2.1958e+00,  1.1502e+00,  8.5943e-01, -1.4726e+00,\n",
       "         -9.3096e-01,  1.3655e+00,  1.5530e+00, -9.4481e-01, -9.1997e-01,\n",
       "          3.7301e-01, -1.6422e+00,  2.7869e-01, -9.3580e-02, -1.6197e+00,\n",
       "          1.7623e+00, -8.1659e-01, -8.7347e-01, -1.4552e+00, -9.1815e-01,\n",
       "          3.1382e-01, -7.5433e-01, -2.6755e+00, -2.7269e-01, -4.5779e-01,\n",
       "          5.6441e-01,  1.8943e+00,  3.1041e-01, -7.5990e-01, -3.7069e-01,\n",
       "         -7.6832e-01, -3.8383e-02, -1.2243e+00, -5.9403e-01,  2.3717e-01,\n",
       "          9.2971e-01,  1.8043e+00,  6.1413e-01,  6.8737e-01,  3.3036e-01,\n",
       "          1.2804e+00,  5.0118e-01,  7.7707e-01,  2.4830e-01, -1.1138e+00,\n",
       "          4.0223e-01,  1.2275e+00, -3.4465e-02,  1.1916e+00, -2.7732e-01,\n",
       "         -5.2761e-01,  3.4210e-01,  1.6522e+00,  9.0783e-01,  2.6652e-01,\n",
       "         -7.5322e-01,  1.0466e+00,  5.3663e-03, -1.0827e+00, -1.1496e+00,\n",
       "         -1.1407e-01, -4.9228e-02, -3.3417e-01, -8.8051e-01, -2.6354e+00,\n",
       "          2.3669e-01,  1.2629e+00,  7.0248e-01,  5.1819e-01, -1.5226e-01,\n",
       "          3.0962e-01,  2.6816e-01, -7.5106e-02, -4.9082e-01, -5.7373e-01,\n",
       "         -5.7587e-01, -7.1407e-01,  3.5299e-01, -4.6493e-01, -2.8861e-01,\n",
       "          2.6218e-01,  5.3344e-01, -4.4895e-01, -2.0326e-01, -6.8831e-01,\n",
       "          3.2694e-01, -1.5618e+00,  8.7306e-01, -9.0481e-01, -2.2755e-01,\n",
       "          2.7467e-01, -3.5234e-01, -1.6707e+00,  6.4135e-02, -5.5185e-01,\n",
       "          3.9574e-01, -6.4956e-03, -8.7102e-01,  1.0765e+00, -3.1762e-01,\n",
       "          1.8550e-01, -5.2992e-01,  3.3149e-01, -2.1306e-01, -1.2937e-01,\n",
       "         -3.6727e-01, -1.8866e-01, -5.8899e-01,  1.1156e+00,  1.1759e+00,\n",
       "          1.7155e+00, -2.0735e+00,  5.2203e-01, -1.3113e+00, -2.0982e+00,\n",
       "         -1.7413e+00, -4.9847e-01, -7.7113e-01,  4.6527e-01, -4.5889e-01,\n",
       "         -1.1565e+00,  1.6529e+00,  1.8466e+00, -5.2485e-01,  2.0121e-01,\n",
       "          8.9002e-01,  1.5185e+00, -1.6215e+00, -9.5530e-02, -5.8894e-01,\n",
       "         -5.7133e-01,  5.6327e-01, -6.9536e-01,  6.8325e-01,  5.0887e-01,\n",
       "         -1.0744e+00,  7.6930e-01,  3.6679e-01,  6.2717e-01, -9.3039e-01,\n",
       "         -2.0671e+00, -1.1230e-01,  6.0882e-01,  1.9172e+00, -6.0286e-01,\n",
       "         -2.6045e-01,  1.6138e-01,  1.0238e+00,  1.5872e-01,  3.9504e-01,\n",
       "          7.0625e-01, -2.6841e+00, -1.0644e+00,  8.4929e-01, -2.1544e-02,\n",
       "         -1.0720e+00, -1.1018e+00, -7.3036e-01, -5.5157e-01, -1.5000e-01,\n",
       "         -1.0433e+00],\n",
       "        [ 4.5445e-01, -1.2229e-01, -9.8729e-01,  1.5101e-01, -2.0248e+00,\n",
       "          1.4953e+00, -7.7271e-01, -8.6041e-01,  2.7107e-01, -1.7257e-01,\n",
       "          7.8077e-01,  1.6261e+00, -8.1999e-01, -1.1722e+00,  8.0455e-01,\n",
       "          1.3531e+00, -1.1178e+00, -6.6646e-02, -7.6418e-01,  5.9716e-01,\n",
       "          1.2865e-01,  6.8202e-01,  8.3924e-02,  8.9490e-01,  2.3040e-01,\n",
       "          3.1165e-01, -8.9308e-01,  1.5574e+00, -5.4153e-01,  7.8424e-01,\n",
       "         -5.8763e-02,  2.7630e-01,  1.7492e-02,  2.3659e-01, -1.2078e+00,\n",
       "         -5.0632e-01,  1.2694e+00,  1.7785e+00, -1.1156e+00, -1.0269e+00,\n",
       "          1.8032e-01,  1.5266e+00,  1.9286e+00, -3.5008e-01, -1.3398e+00,\n",
       "         -1.9536e-03, -3.1489e-01,  9.6128e-01,  1.8773e-01, -9.4299e-02,\n",
       "         -2.9338e-01, -6.3466e-01,  5.3505e-01,  7.3121e-02, -6.8371e-01,\n",
       "          1.5996e-01,  1.2253e+00, -7.4306e-01,  1.1937e-01,  2.6567e+00,\n",
       "         -4.2671e-01, -4.6915e-01, -5.1082e-01,  1.9400e-01,  4.3176e-01,\n",
       "          4.1404e-01,  8.7659e-02, -2.3021e+00,  6.3665e-01, -6.0378e-01,\n",
       "          3.3157e-01,  1.0630e+00,  2.7284e-01, -1.5510e+00,  5.1931e-01,\n",
       "         -1.4751e+00, -1.7738e+00, -8.4205e-01, -1.4848e+00, -1.2621e+00,\n",
       "         -1.4010e+00,  4.7992e-02,  8.5946e-01, -9.4581e-01, -8.4895e-01,\n",
       "          8.2386e-01, -1.2921e+00,  2.6167e-01,  3.2896e-01,  1.6484e+00,\n",
       "         -1.1450e+00, -5.1121e-01, -1.0027e+00, -1.1507e+00,  8.5424e-01,\n",
       "          1.6808e+00, -8.2161e-01,  5.2244e-01, -5.4114e-01,  1.8758e+00,\n",
       "         -6.4018e-01, -3.5359e-01, -1.0822e+00,  4.4668e-01, -1.4921e-01,\n",
       "         -1.5316e-01,  9.0757e-01, -3.4006e-01, -2.4481e-01, -9.0057e-01,\n",
       "          4.2139e-01, -2.6638e-01, -9.5713e-01,  4.1894e-01, -4.8419e-01,\n",
       "          2.8510e+00, -5.6135e-01, -2.8104e+00,  1.4794e+00,  3.2462e-02,\n",
       "         -1.2269e+00,  8.3590e-01,  1.5284e+00, -4.0661e-01,  9.8620e-01,\n",
       "          4.8611e-01,  1.1883e+00,  1.8971e-01,  2.5642e+00,  6.7671e-02,\n",
       "          4.4729e-01,  9.2798e-01, -8.5723e-01, -5.1698e-01,  3.6864e-01,\n",
       "         -5.8566e-01,  2.1425e+00,  9.9362e-01, -9.9030e-01,  4.1312e-01,\n",
       "         -8.7452e-01,  1.4118e-01, -1.1880e+00, -1.6625e-01, -3.0288e+00,\n",
       "         -4.6511e-02,  1.1107e+00, -1.4625e+00, -1.0923e+00, -1.2048e+00,\n",
       "         -3.9210e-01, -6.9490e-01, -5.1388e-01,  4.5452e-01,  6.8283e-01,\n",
       "          4.6076e-01, -2.0284e+00, -1.5741e-01, -1.5221e-01, -5.2036e-01,\n",
       "          8.8093e-01,  4.4602e-01, -1.4690e+00, -2.8215e-02, -4.2500e-01,\n",
       "         -1.4386e-01,  9.2164e-01,  7.1084e-01, -1.0126e+00,  2.7743e+00,\n",
       "          1.3613e+00, -1.0750e+00,  7.1431e-01,  1.3304e-01, -1.5033e+00,\n",
       "         -2.7919e-01,  1.0765e+00,  1.5240e+00,  1.6216e+00,  1.2158e+00,\n",
       "          9.3288e-01,  2.2313e+00, -9.6020e-01,  1.5298e+00,  1.4578e+00,\n",
       "         -2.5674e+00,  9.8496e-01, -1.3539e+00,  1.0471e+00,  1.2609e+00,\n",
       "         -1.7565e-01, -9.1944e-01,  3.9181e-01,  1.2104e+00, -6.1706e-01,\n",
       "         -2.0950e-01,  6.6680e-01, -5.1187e-01, -4.9664e-01,  1.0261e+00,\n",
       "          3.7599e-01,  4.0368e-01, -1.1891e+00,  1.4373e+00,  6.4077e-01,\n",
       "          2.9934e-02, -2.8620e-01,  6.4344e-01, -1.9184e-01, -1.7461e-01,\n",
       "         -8.8608e-01, -6.5019e-01, -1.6348e+00, -8.1173e-01, -3.0811e-01,\n",
       "          6.3721e-01, -4.0938e-01,  4.7279e-01, -8.6741e-01,  2.3199e+00,\n",
       "         -1.8089e+00, -2.1521e-01, -1.3875e+00, -2.6247e+00, -2.4218e+00,\n",
       "         -8.9547e-01,  3.3064e-01,  1.1208e+00, -3.2789e-01,  8.6060e-01,\n",
       "          5.9601e-01, -3.3536e-01, -5.6219e-01, -3.2505e-01,  2.5237e-02,\n",
       "         -2.0669e+00, -3.0329e-01,  2.5318e-01,  1.5712e+00, -2.4610e-01,\n",
       "          4.4285e-01,  6.8170e-02,  1.2177e+00, -1.0761e+00,  1.4924e-01,\n",
       "         -5.1369e-01, -1.8330e+00, -1.8203e+00, -1.7181e+00, -1.9012e-01,\n",
       "         -1.1810e-01,  3.5629e-01,  2.2363e+00,  1.1486e+00,  4.0014e-01,\n",
       "          9.7969e-02],\n",
       "        [-2.4215e-01, -5.1921e-01,  3.5417e-02, -1.4555e+00,  1.4076e+00,\n",
       "          6.6691e-01, -8.1197e-01, -1.2249e+00,  3.6602e-01,  8.1733e-01,\n",
       "          1.2094e+00, -6.0523e-02, -4.2125e-01, -7.8057e-01,  5.5776e-01,\n",
       "          4.8376e-01,  1.3027e-01,  6.5080e-02,  1.3562e+00,  5.8926e-01,\n",
       "         -5.1709e-01,  1.6519e+00, -9.9432e-01,  4.6230e-01, -1.8293e-01,\n",
       "         -1.3788e+00,  9.4772e-01,  2.2415e-01,  6.1789e-01,  1.8664e+00,\n",
       "          1.0346e+00, -2.2112e+00,  4.2181e-01, -1.0195e+00,  1.8576e+00,\n",
       "          7.6418e-01,  6.2753e-01,  5.9313e-01, -9.9451e-01, -2.5246e+00,\n",
       "          9.4990e-01, -6.3309e-01, -1.6368e+00,  1.8838e+00,  6.3872e-01,\n",
       "         -3.2595e-01,  4.3838e-01, -8.0652e-01,  6.1715e-01, -1.3943e-01,\n",
       "         -4.5837e-01, -6.2493e-01, -1.0240e+00,  1.3237e+00,  5.2770e-01,\n",
       "         -1.1032e+00, -1.2592e+00, -5.5044e-01,  1.7943e-01,  1.3206e+00,\n",
       "          4.1296e-01,  1.0447e-01, -3.8304e-01, -7.6879e-01, -1.8390e+00,\n",
       "          1.4027e+00,  1.3034e+00, -2.3173e-01, -2.6899e-01, -3.8271e-01,\n",
       "         -6.5670e-01, -1.0517e+00,  2.5318e-01,  2.0017e-01,  9.0227e-01,\n",
       "         -1.9380e-02, -2.0400e+00,  1.8076e+00, -6.2961e-01,  2.3600e-01,\n",
       "          8.7741e-01, -1.5660e-01,  7.6925e-01,  1.4708e-01, -1.8873e+00,\n",
       "          9.5468e-01, -1.3410e+00, -6.2296e-01, -1.4289e+00,  1.9698e+00,\n",
       "          9.6103e-01, -2.6032e-01,  3.7873e-01, -1.2986e+00,  1.0916e+00,\n",
       "         -1.9685e-01, -8.5815e-01,  1.1668e+00, -3.7291e-01, -4.9141e-01,\n",
       "          1.8562e+00, -4.0073e-01,  2.3390e-02,  6.1813e-01, -6.4011e-01,\n",
       "         -1.0347e+00, -1.8264e+00,  6.8984e-01, -8.4064e-01,  5.0680e-01,\n",
       "         -1.3550e+00,  1.0496e+00,  1.3508e+00, -2.1094e+00,  4.4101e-01,\n",
       "          2.0142e+00,  1.5071e+00, -2.1376e+00, -8.0053e-01,  5.3775e-01,\n",
       "          2.1629e-01, -9.7029e-01,  5.4518e-01, -1.1821e+00, -2.1450e-01,\n",
       "          5.2340e-01,  5.6742e-01, -6.7817e-01, -1.8232e+00,  1.2055e-01,\n",
       "         -4.5112e-01,  5.3932e-01, -3.9232e-01,  6.5119e-02,  4.2481e-01,\n",
       "         -7.1413e-01,  1.7602e+00, -7.7509e-01,  4.0161e-01, -4.5451e-01,\n",
       "          1.4660e+00,  4.3385e-01,  1.4359e+00,  6.2381e-01,  7.5751e-01,\n",
       "          5.2935e-01, -5.7452e-01, -5.1636e-01,  1.5839e+00,  4.3548e-01,\n",
       "          1.7881e+00,  4.9381e-02, -1.6004e+00, -9.4600e-01, -4.9426e-01,\n",
       "          8.4708e-01, -3.0448e-01, -1.0963e+00,  1.4326e+00,  1.0499e-01,\n",
       "         -4.3317e-01,  1.2107e-01,  7.2746e-01, -5.6368e-01, -1.6274e+00,\n",
       "          3.4949e-01,  2.3135e-01,  7.7934e-01,  6.1774e-02, -2.6060e-01,\n",
       "         -4.9539e-01,  2.9953e-01,  5.3905e-02, -5.7009e-01,  2.0406e+00,\n",
       "          1.8403e+00, -1.2466e-01,  3.3083e-01, -1.0022e+00, -9.3449e-01,\n",
       "         -6.4264e-01, -7.7324e-02,  9.7997e-01, -1.3281e+00, -4.2466e-02,\n",
       "         -5.3501e-01,  1.3260e+00,  1.6415e+00, -2.5915e-01, -4.2522e-01,\n",
       "          1.0481e+00, -1.6274e+00,  3.4040e-01,  3.8888e-01, -5.0038e-01,\n",
       "         -1.3859e+00, -5.5628e-02,  9.7832e-01, -2.2535e+00,  2.4952e-01,\n",
       "          4.9860e-02, -1.2903e+00,  6.8877e-01, -6.8642e-01, -1.9217e+00,\n",
       "          5.7380e-01, -3.9030e-01, -1.1598e-01,  9.7044e-01,  1.5220e+00,\n",
       "         -1.5338e+00, -6.5751e-01,  3.5239e-01, -2.1284e+00, -3.4801e-01,\n",
       "         -1.3700e+00, -3.3852e-01,  1.4425e+00,  1.5042e+00, -1.7341e-01,\n",
       "         -1.4197e+00, -6.1839e-01,  1.0619e+00,  1.4752e+00,  1.5716e+00,\n",
       "         -9.5334e-01,  3.5006e-01,  9.3926e-01,  1.1819e+00, -5.9459e-01,\n",
       "         -9.0234e-01, -7.9237e-01,  7.2437e-01,  1.8050e+00, -8.2375e-01,\n",
       "          1.0606e+00, -7.7523e-01, -1.0106e+00, -1.1643e+00, -8.5188e-01,\n",
       "          1.2652e+00,  7.1161e-01,  6.2156e-01,  7.7793e-01,  2.1976e-01,\n",
       "          9.3559e-01,  5.0516e-01, -7.5658e-01,  6.6147e-01,  4.4001e-01,\n",
       "          7.6644e-01, -2.6059e-01,  1.2675e+00,  1.4929e+00,  8.6052e-01,\n",
       "         -3.6001e-01],\n",
       "        [ 2.0823e+00,  4.7354e-01,  8.5138e-01,  7.6715e-01,  5.1856e-01,\n",
       "          7.8822e-01,  1.7712e+00,  7.8615e-01, -3.9587e-02,  4.4441e-01,\n",
       "         -8.2313e-02,  7.5720e-01, -1.3795e+00, -6.3159e-01, -1.7966e+00,\n",
       "         -1.1621e+00, -5.8824e-01, -6.5051e-01, -6.0334e-01,  2.2491e-01,\n",
       "          4.8966e-01,  9.7697e-01,  1.1447e+00,  7.3332e-01, -7.5799e-03,\n",
       "         -5.4930e-01, -9.3717e-01,  9.1303e-01, -1.7407e-01, -6.3733e-01,\n",
       "          4.4515e-01,  5.3302e-01, -2.2951e+00,  1.2451e+00, -7.9918e-01,\n",
       "          1.4447e+00, -1.2508e+00,  8.9385e-01, -4.2998e-01, -6.3389e-01,\n",
       "          5.3881e-01,  9.7609e-01, -2.5336e-01,  3.1070e-01,  3.5218e-01,\n",
       "          1.0986e+00,  4.9938e-01,  7.7742e-01, -5.0254e-03, -4.3397e-01,\n",
       "          1.6004e+00, -1.5684e-01, -1.1290e+00, -3.1217e-01, -2.1004e-01,\n",
       "          1.6382e+00, -2.1672e+00,  4.4238e-01, -7.5337e-02,  1.0659e+00,\n",
       "         -7.8320e-01,  6.9041e-01,  2.6194e-01, -1.0652e+00, -7.8340e-01,\n",
       "          2.3614e+00,  1.4228e+00,  1.0159e+00, -8.2799e-01, -1.0429e+00,\n",
       "         -6.6585e-01, -2.4020e-01, -1.0013e+00, -3.2595e-01,  1.1251e+00,\n",
       "          5.0693e-01, -9.9359e-01, -1.2718e+00, -6.5554e-01,  2.2277e-01,\n",
       "          1.9951e+00, -1.6318e+00,  1.6482e-02, -6.4214e-01, -2.5358e+00,\n",
       "          1.2289e+00, -4.1914e-01, -8.5777e-01,  1.5607e+00, -2.2214e+00,\n",
       "          2.7165e-01, -2.1199e+00,  4.6211e-01,  6.6224e-01,  9.2211e-01,\n",
       "         -5.8699e-01,  9.9014e-01, -2.2935e-02, -1.0958e+00,  1.1241e+00,\n",
       "          3.2063e-01, -9.9949e-01,  6.2491e-01,  1.5241e+00,  7.3685e-01,\n",
       "         -1.7197e+00,  2.4171e-01,  7.6829e-01, -9.2441e-01, -4.3340e-01,\n",
       "         -8.1202e-01, -1.8439e+00,  5.5686e-01, -9.0804e-01,  7.2805e-01,\n",
       "         -6.3007e-01, -4.6877e-01, -6.6604e-02,  1.2991e+00, -4.7291e-01,\n",
       "          3.2288e-01,  3.0271e-01,  1.0134e-01,  3.0861e-02, -8.5937e-01,\n",
       "         -2.9590e-01, -2.4755e-01,  3.7225e-01, -1.4734e+00,  1.4462e+00,\n",
       "          2.6093e+00, -3.4624e+00,  8.9063e-01, -5.9941e-01, -5.6276e-01,\n",
       "          6.1555e-01,  2.2242e+00,  1.3489e+00, -1.1960e+00, -4.3187e-01,\n",
       "         -1.1237e+00,  1.4686e+00,  1.5876e-02,  1.3531e+00,  1.5759e-01,\n",
       "         -1.3238e+00, -8.5694e-01,  6.5312e-01, -8.8189e-01,  1.2380e+00,\n",
       "         -9.4100e-01, -2.6948e-01, -8.5106e-01, -1.3690e-01,  8.4518e-01,\n",
       "          1.8625e-01, -1.0800e+00,  1.6670e+00, -1.7072e+00, -9.4752e-01,\n",
       "          6.8809e-01, -4.7777e-01,  1.8679e+00,  2.5800e-01,  9.3287e-01,\n",
       "         -1.4018e+00, -1.8480e-01,  1.4244e-01, -6.9471e-01, -1.5683e+00,\n",
       "          1.1566e+00, -1.2316e+00, -1.0700e+00,  1.0089e+00, -5.7362e-01,\n",
       "         -1.3383e+00, -3.4667e-01,  3.2691e-01,  8.4682e-02,  3.1251e-01,\n",
       "          2.1554e+00,  8.9494e-01, -8.3142e-01,  6.6788e-01,  6.8751e-01,\n",
       "         -2.2416e+00, -9.3690e-01, -1.6614e-01,  1.0113e-01,  5.0221e-01,\n",
       "          1.0558e+00, -4.1428e-01, -1.0206e+00,  1.6538e-01, -1.6759e-02,\n",
       "          6.6232e-01, -6.3886e-01, -6.7654e-01, -1.8934e+00, -1.0078e+00,\n",
       "         -2.6596e+00,  8.8347e-01, -6.3258e-01, -2.7029e-01,  1.5595e+00,\n",
       "         -1.9238e-01,  1.6921e+00,  9.7747e-01, -3.7908e-01, -2.9472e-01,\n",
       "          4.5380e-01, -5.0855e-01, -3.6241e-01,  8.2399e-01,  2.1418e+00,\n",
       "         -1.7325e+00,  7.1941e-01,  4.0045e-01,  1.3163e+00,  1.4480e+00,\n",
       "         -5.9967e-01, -1.5370e-01, -1.9208e-01,  5.3442e-01, -1.1716e-01,\n",
       "         -6.6962e-01, -8.8994e-01,  2.6059e-01, -3.7111e-01,  1.2433e-01,\n",
       "         -3.0079e-01, -4.0240e-02, -1.7914e-01,  7.5379e-01, -1.9789e+00,\n",
       "         -2.5357e-01,  6.6613e-01,  1.1161e-01,  8.4128e-01,  3.9412e-01,\n",
       "         -1.9237e+00, -3.8088e-01,  3.7928e-01, -3.6688e-01,  1.5208e-01,\n",
       "          3.2956e-01,  8.4126e-01,  8.0038e-01,  9.3111e-01, -9.0657e-02,\n",
       "          8.1696e-01, -1.7929e+00, -6.5250e-01, -1.6675e+00,  3.8551e-01,\n",
       "          7.9594e-02]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just look at the matrix . It is of the size same as token embedding\n",
    "# this will be now added to each of the elements in token embedding \n",
    "torch.set_printoptions(profile=\"full\")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82329b44-60a2-4e48-98cf-487136541bc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b512c53b-6aab-4d04-aa6c-b535e2012db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "340b47dd-6e2c-4590-9dbc-59a0c86fccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = all_embeddings + pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5f2f931-0691-483e-bd31-87233a57610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a49b02-c09d-4c82-b1ac-1cb5cd361985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
