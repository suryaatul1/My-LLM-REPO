{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d8a076-059f-49f7-8dbb-f44feafc5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362f529-e87a-4484-bf32-40983f064b58",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Self Attention WIth Trainable Weights</b> \n",
    "<p>    \n",
    "\n",
    "After token embedding , we are able to project the tokens (words) in this sentence into higher dimension vector space.\n",
    "\n",
    "We can build and understand the semantics and meaning of the individual works .\n",
    "However the problem arise , we are still not able to build the relationship or context of particular word with the overall sentence /paragraph \n",
    "\n",
    "This is why we need attention mechanism , to build the relationship of each token with each other token . meaning how much attention has to paid to other tokens with respect the one in question\n",
    "\n",
    "Last time we saw the simplified ateention mechanism.\n",
    "Now in this notebook , we extend the same attention mechanism with trainable Weights. These Weights will be trained and optimized during pre-training the LLM/\n",
    "\n",
    "In this as compared to previoue , we introduce 3 weight matrices know as\n",
    "1. Query :- Wq\n",
    "2. Key :- Wk\n",
    "3. Value:- Wv\n",
    "\n",
    "Lets see below what it does\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bbb91b4-8621-4eca-ace4-86c6cc8ac0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image this is the Token embedding for the input sentence = \"your joruney starts with one step\"\n",
    "\n",
    "inputs = torch.tensor(\n",
    " [[0.43, 0.15, 0.89],  # Your\n",
    " [0.55, 0.87, 0.66],  # journey\n",
    " [0.57, 0.85, 0.64],  # starts\n",
    " [0.22, 0.58, 0.33],  # with\n",
    " [0.77, 0.25, 0.10],  # one\n",
    " [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a79fb3a7-29ac-4d52-90a4-72badca080be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initials someabs\n",
    "\n",
    "test_token = inputs[1] # lets pick journey token to see how this works\n",
    "d_in = inputs.shape[1]\n",
    "d_out=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc71d6-0a76-4fac-a72b-57e4e091601a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Initial the Weight Matrices</b> \n",
    "<p>    \n",
    "\n",
    "In self attention mechanism  , we introduce 3 weight matrices know as\n",
    "1. Query :- Wq\n",
    "2. Key :- Wk\n",
    "3. Value:- Wv\n",
    "\n",
    "Lets initialize them with some random values\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69eaff2-9bd2-43ac-b9ac-b26696d0caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in , d_out) , requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in , d_out) , requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in , d_out) , requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa11cc6-b544-4f90-a7bd-730a8a9cb1c8",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "This is for the Token 2\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982b616f-c1d8-4d3e-b5e6-8c6f6676c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform matric multiplication of these initializated Weight Matrix\n",
    "# We will do it for 2nd Token that is journey\n",
    "# this is to generated the new matrix query , key and value for 2nd token\n",
    "\n",
    "query_token_2 = test_token @ W_query\n",
    "key_token_2 = test_token @ W_key\n",
    "value_token_2 = test_token @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d1b066-9790-4523-a293-74a4a52ec19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Matrix: tensor([0.4306, 1.4551])\n",
      "Key Matrix: tensor([0.4433, 1.1419])\n",
      "Value Matrix: tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "print(\"Query Matrix:\",query_token_2)\n",
    "print(\"Key Matrix:\",key_token_2)\n",
    "print(\"Value Matrix:\",value_token_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e41baa-3465-410d-97da-5b4e3ed3c7be",
   "metadata": {},
   "source": [
    "---\n",
    "<font color='blue'>\n",
    "Lets do it fore all the Tokens\n",
    "\n",
    "We create the query , key , value matrix for all the input tokens\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433a5779-cf21-40c0-b7c3-40ff2ecca954",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs @ W_query\n",
    "key   = inputs @ W_key\n",
    "value = inputs @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6689535-eba6-4584-819b-f15bc1d2bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Matrix: torch.Size([6, 2])\n",
      "Key Matrix: torch.Size([6, 2])\n",
      "Value Matrix: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Query Matrix:\",query.shape)\n",
    "print(\"Key Matrix:\",key.shape)\n",
    "print(\"Value Matrix:\",value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec7c56-e299-4c82-a108-9cf3032171e1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Calculate Attention Score</b> \n",
    "<p>    \n",
    "\n",
    "Now we calculate the attention score for each token , to see how they relate to the other tokens in the sentence\n",
    "\n",
    "Basically the attention score matrix looks like below\n",
    "\n",
    "token1 = [val1, val2, val3, val4, val5, val6]\n",
    "\n",
    "token2 = [val1, val2, val3, val4, val5, val6]\n",
    "\n",
    "token3 = [val1, val2, val3, val4, val5, val6]\n",
    "\n",
    "token3 = [val1, val2, val3, val4, val5, val6]\n",
    "\n",
    "\n",
    "The columns again map to same number of tokens.\n",
    "This is like a corelation matrix between each tokens . \n",
    "Higher the value more they corelate. \n",
    "[0,0] will have high corelation becuase it is between same token1 .\n",
    "\n",
    "Likewize all the diagonal values will be highly corelated\n",
    "\n",
    "However in this we calculate the attention score using query matrix and key amtrix for each token\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dce20fb-a6fd-4544-aa1d-c8d9d09c9c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "# lets calculate the attention score for all tokens\n",
    "# Dimension for query and ket is 3 * 2 \n",
    "\n",
    "attention_score = query @ key.T\n",
    "print(attention_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8ed49-82a4-4558-a8ab-e70af44f2483",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Normalize the  Attention Score and divide by sqrt of </b> \n",
    "<p>    \n",
    "\n",
    "The attention scores , needs to be interpreted in probabilities . Meaning the values of attention score for each token should be between 0 and 1 , and should be able to tell the weightage to be give in %\n",
    "\n",
    "<font color=red> IMP** \n",
    "As compared to the  previous simplified self attention method , this method of nomralization has additional step of dividing the attention scores by the square root of embedding dimension of keys\n",
    "</font>\n",
    "\n",
    "This is the reason it is called scaled self attentions\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84a17b7e-6778-4867-83d4-4892c12d0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform normalization and also divid the attention score as said earlier\n",
    "\n",
    "keys_dimension = key.shape[1]\n",
    "attention_score_norm_step1 = attention_score / keys_dimension**0.5\n",
    "attention_score_norm = torch.softmax(attention_score_norm_step1 , dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11fa4fb6-b221-445a-9952-5a95ddc8daac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
       "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe8f73-f133-4178-bb98-301d4fc1a662",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Why Divide by sqrt </b> \n",
    "<p>    \n",
    "\n",
    "<font color=red> IMP**\n",
    "\n",
    "1. softmax kinda gives peak value for higher dot product\n",
    "2. srqt allow to kinf the variance close to 1 , when the dimensions increase\n",
    "\n",
    "check :- youtube \n",
    ":- https://www.youtube.com/watch?v=UjdRN80c6p8&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=15 for more details at time 44:45\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2aa09e-515b-41ce-bd1c-8e92076383fe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cf1655f-6be5-4f03-9e9b-386cbb30a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = attention_score_norm @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2edd2b7-44ab-4d27-a0e2-042577574bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140685e-1d51-49fa-8c52-975efc83f782",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2978fc1-9057-4806-9903-eea5ae574fdf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Self Attention Python CLass</b> \n",
    "<p>    \n",
    "Let pack this in a python class.\n",
    "    It make it simple\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26846551-fe0b-464d-9e77-5a583eccdc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05ea17c9-9c3a-4554-bf81-9c3d8b2a0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_int , d_out):\n",
    "        super().__init__()\n",
    "\n",
    "        # Step 1 :- Define the query , key and valye matrices with randowm values\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # STep2 : create the key , queries and values by multiplying token embedding\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        # Step 3 : Create attention scores \n",
    "        attention_scores = queries @ keys.T\n",
    "\n",
    "        # Step 4 :- Normalize the attention scores by diding then with sqrt od dims and softmax\n",
    "        attention_scores_norm = torch.softmax(\n",
    "              attention_scores / keys.shape[1]**0.5 , dim=-1\n",
    "            )\n",
    "        \n",
    "        # Step 5: Create context vector\n",
    "        context_vector = attention_scores_norm @ values\n",
    "        return context_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a23361db-d4ce-4fcd-8d89-24bfabfd16de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the shape and size of query , keys and values matrices\n",
    "\n",
    "torch.manual_seed(123)\n",
    "d_in = inputs.shape[1]\n",
    "d_out=2\n",
    "\n",
    "sa_v1= SelfAttention_v1(d_in , d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831a962-bda8-44f4-9b87-d2907ffd858b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Self Attention Python Class Version 2</b> \n",
    "<p>    \n",
    "We used the <font color=red> nn.parameters </font> to initial the weights for Query, key, and Values matrices.\n",
    "    However the more preferred way is to use the <font color=red> Linear function</font>. \n",
    "    Below are some of the benefits of using Linear over parameters\n",
    "\n",
    "Using nn.Linear over nn.Parameter provides several benefits:\n",
    "\n",
    "\n",
    "1. Automatic weight and bias initialization: nn.Linear initializes weights and biases using best practices, reducing manual errors.\n",
    "2. Built-in forward computation: It handles matrix multiplication and bias addition, simplifying your code.\n",
    "3. Parameter management: Parameters are automatically registered for optimization and device management.\n",
    "4. Consistency: Using standard layers makes your code more readable and maintainable.\n",
    "Integration: Works seamlessly with other PyTorch modules and utilities (e.g., model saving/loading).\n",
    "\n",
    "Overall, nn.Linear is more convenient and less error-prone for defining learnable linear transformations.\n",
    "    \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9911648c-d82c-4da7-99b5-e5e7bff90c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_int , d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Step 1 :- Define the query , key and valye matrices with randowm values\n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # STep2 : create the key , queries and values by multiplying token embedding\n",
    "        keys    = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values  = self.W_value(x)\n",
    "\n",
    "        # Step 3 : Create attention scores \n",
    "        attention_scores = queries @ keys.T\n",
    "\n",
    "        # Step 4 :- Normalize the attention scores by diding then with sqrt od dims and softmax\n",
    "        attention_scores_norm = torch.softmax(\n",
    "              attention_scores / keys.shape[-1]**0.5 , dim=-1\n",
    "            )\n",
    "        \n",
    "        # Step 5: Create context vector\n",
    "        context_vector = attention_scores_norm @ values\n",
    "        return context_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "892d787a-5a31-4dcb-b36a-f466a34a2429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the shape and size of query , keys and values matrices\n",
    "\n",
    "torch.manual_seed(789)\n",
    "d_in = inputs.shape[1]\n",
    "d_out=2\n",
    "\n",
    "sa_v2= SelfAttention_v2(d_in , d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aceef52-b60c-4731-88ed-79c2a8d50f5c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Why QUERY, KEY and VALUE</b> \n",
    "<p>    \n",
    "Consider below analogy\n",
    "\n",
    "QUERY :- It is pretty much like query in a database. It is the current token the model focus on\n",
    "\n",
    "\n",
    "KEY :- Each item in input sequence has a KEY . This KEY are ysed to match with the query\n",
    "\n",
    "VALUE :- Once the KEY is identified , we retrive the value associated with the KEY that is most relevant to the query\n",
    "    \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf2bc8-bced-44eb-addc-dee779fd0e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
