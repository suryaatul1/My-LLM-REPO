{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94e1faa-695e-418a-a9cd-8e31ded85430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8adf6ee9-2d90-42d6-8167-2cb1284082c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19c7054e-6adb-498a-af15-2ff35fee0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    #print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed265b1-0b47-4479-a0be-9b3a064ae8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,deque\n",
    "from functools import  lru_cache\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05740602-a262-4d9c-994a-2fa2142f5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        self.vocab={}\n",
    "        self.inverse_vocab={}\n",
    "        self.bpe_merges={}\n",
    "        self.bpe_ranks={}\n",
    "\n",
    "    def train(self,text,vocab_size,allowed_special={\"<|endoftext|>\"}):\n",
    "        processed_text1=[]\n",
    "        for i , char in enumerate(text):\n",
    "            #print(\"char is:\",char)\n",
    "            if char==\" \" and i != 0:\n",
    "                processed_text1.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text1.append(char)\n",
    "            processed_text=\"\".join(processed_text1)\n",
    "            #print(\"processed Text is :\",processed_text)\n",
    "\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "              char for char in sorted(set(processed_text))\n",
    "              if char not in unique_chars                \n",
    "            )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "        #print((unique_chars)\n",
    "\n",
    "        self.vocab={i: char for i , char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab={char :i for i, char in self.vocab.items()}\n",
    "\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id=len(self.vocab)\n",
    "                    self.inverse_vocab[token]=new_id\n",
    "                    self.vocab[new_id]=token\n",
    "           \n",
    "        #print(self.vocab)  \n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "        #absprint(token_ids)\n",
    "        print(len(self.vocab))\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        @staticmethod\n",
    "        def replace_pair(token_ids, pair_id, new_id):\n",
    "          dq = deque(token_ids)\n",
    "          replaced = []\n",
    "\n",
    "          while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f938016c-df1c-4416-a337-9170fd7f11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpetoknizer = BPETokenizerSimple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63d5c89a-d8fa-4c6c-9c5b-ec29bb9ad87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BPETokenizerSimple' object has no attribute 'find_freq_pair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbpetoknizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe fox is out in the wild\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 43\u001b[0m, in \u001b[0;36mBPETokenizerSimple.train\u001b[0;34m(self, text, vocab_size, allowed_special)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab), vocab_size):\n\u001b[0;32m---> 43\u001b[0m     pair_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_freq_pair\u001b[49m(token_ids, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pair_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BPETokenizerSimple' object has no attribute 'find_freq_pair'"
     ]
    }
   ],
   "source": [
    "bpetoknizer.train(text=\"The fox is out in the wild\", vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16def5b5-60eb-4b8c-91f4-5053816ecaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53686e35-3abd-42d4-a04b-47a3c40b290e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
