{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92be03aa-348a-4df0-ab12-b6c7110ea4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in data: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\" , \"r\" ,encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "\n",
    "print(\"Total number of characters in data:\" , len(raw_text))\n",
    "print(raw_text[:99])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312d813-68c4-4cf5-845e-b9ecbadf38f9",
   "metadata": {},
   "source": [
    "# Step 1 : Tokenzing the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5af6e-7d81-4907-9921-dee65e66c12d",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#FF5733\">\n",
    "We now create tokens using the regex that splits on whilespace. \n",
    "Also include the \".\" and \",\"\n",
    "We then strip the whitespaces from the sentence for number or reason (size, memory , processing power)\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2135e331-dd74-4436-a743-8116e26b2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text=\"Hello, world. Is this-- , a test?\"\n",
    "re_words=re.split(r'([,.:;?_!\"()\\']|--|\\s)' , text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48710855-f71c-4793-88b0-8ce465d1cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', '', ',', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "print(re_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d044cf0-e5f2-4878-b9ad-8580e99b43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = [item for item in re_words if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73eaade-2f91-4ee2-894d-c326dcff3856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', ',', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb02013-42dc-48b5-bdaa-21a484ecb516",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#FF5733\">\n",
    "This was example.\n",
    "    Now below we tokenizing the actual dataset\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791a9a25-7686-4569-a3f5-ca1629addbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed=re.split(r'([,.:;?_!\"()\\']|--|\\s)' , raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639e740d-ecfb-4a4d-8c67-359135eba049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8554551-f126-4ebb-a9bd-f71b39c573d8",
   "metadata": {},
   "source": [
    "# Step 2 : Assign Token to Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e59a2c-76f1-4005-8022-905a39e33661",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#FF5733\">\n",
    "Now we create a vocabulary from the text , and then assign then numeric id values\n",
    "    1. We first create set to get unique values in vocab\n",
    "    2. Then we sort them\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce96e1e-3217-4120-b7a4-73f1c8793d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words=sorted(set(preprocessed))\n",
    "\n",
    "# the size of the vocabulary from the training set we choose to us to pre-train our LLM\n",
    "\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0699120-053a-40c4-be46-d1552655bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we enumarate then to create a token and word combination \n",
    "# Store them in a dictionary  , and this is ove vocabulary \n",
    "\n",
    "vocab = {token:index for index,token in enumerate(all_words)}\n",
    "\n",
    "#for i,word in enumerate(sorted(set(preprocessed))):\n",
    "#    print(\"Index :\" ,i , \"words:\",word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f97e2d23-580b-4c23-89d2-3cda139aed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n"
     ]
    }
   ],
   "source": [
    "for i , item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >=10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a3d1b-39e9-4664-b715-81a5365967f4",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#FF5733\">\n",
    "Now we create a Tokenizer Class for input Text that comes on the fly.\n",
    "    This will contain 2 methods encode and decode \n",
    "\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729bbc7e-a60a-49b1-adc1-ee6b2de2b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int =vocab\n",
    "        self.int_to_str={index:token for token ,index in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s]  for s in preprocessed ]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        list_of_text=[self.int_to_str[id] for id in ids]\n",
    "        text = \" \".join(list_of_text)\n",
    "        text = re.sub(r'\\s+([,.?_!\"()\\'])',r'\\1' ,text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91859753-708f-4b9a-bffe-8a3c81a4b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=SimpleTokenizerV1(vocab)\n",
    "text=\"\"\"\"It's the last he painted, you know,\"\n",
    "         Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54fe2a81-6f4e-4445-8131-bd1ff5a5d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "text=tokenizer.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6b4e6-cdf6-467e-a929-955c9d0b7c73",
   "metadata": {},
   "source": [
    "# ADDING  SPECIAL CONTEXT TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767b327-09a0-49d0-b516-8cb129704659",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#FF5733\">\n",
    "What happens when LLM comes across a word that is not part of the vocabulary we generated from the training \n",
    "    datasets . We cannot have LLM fail due to this . This scenarios is handled by Adding special context\n",
    "tokens\n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d8494-0786-4a90-848e-ebee36daac72",
   "metadata": {},
   "source": [
    "\n",
    "In this case we will add additional tokens to vocab.\n",
    "1. <|unk|>  :- This will be used when there is a word/token encountered that is not part of vocab\n",
    "2. <|endoftext|> :- LLM are trained on large corpus of datasets (multiple books  , articles etc ) lets call this training set. Now to clearly\n",
    "                        distinguish between these different training sets , we add this token before each training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d8e4e94-25a4-40b3-b3a8-0edd1ffce0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ae9064a-c0e0-459e-874d-816aff02f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens=list(sorted(set(preprocessed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e399af-218c-40bf-9c88-a0ec0bf10571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W We sort and convert them to list . Add new tokens using extend as we want this at the end of vocab list\n",
    "all_tokens.extend([\"<|endoftext|>\" , \"<|unk|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe54c1f-325a-4e5c-b600-719c97b5729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:index for index,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b483ee7e-cec1-4a48-8121-193c52ec4c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n"
     ]
    }
   ],
   "source": [
    "for i , item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >=10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "106c876e-6f2f-4127-91ff-b3b538b1faf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('younger', 1127),\n",
       " ('your', 1128),\n",
       " ('yourself', 1129),\n",
       " ('<|endoftext|>', 1130),\n",
       " ('<|unk|>', 1131)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print last 5 tokens \n",
    "\n",
    "list(vocab.items())[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38f505-b015-47ab-956b-e2aad03afabf",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#FF5733\">\n",
    "We now create a new version on SimpleTokenizer Class as SimpleTokenizerV2.\n",
    "As part of updates to this class , we make sure that any new toekn/word that is not part of the vocab we created\n",
    "will be replace with new token \"<|unk|>\" , the one we added above. This way even if the token/word is not part\n",
    "dataset/training set we will not LLM failing\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d58eea10-5fe1-4553-a35b-c36ea9738074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int =vocab\n",
    "        self.int_to_str={index:token for token ,index in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)' , text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# This is the comprehension way\n",
    "        prepdata = [\n",
    "                     item if item in self.str_to_int \n",
    "                     else \"<|unk|>\" for item in preprocessed\n",
    "                   ]\n",
    "\n",
    "        \n",
    "# This is one way a classic way to add new token for tokens not part of vocab\n",
    "#        prepdata=[]\n",
    "#        for item in preprocessed:\n",
    "#            if item in self.str_to_int:\n",
    "#                prepdata.append(item)\n",
    "#            else:\n",
    "#                prepdata.append(\"<|unk|>\")\n",
    "               \n",
    "\n",
    "        ids = [self.str_to_int[s]  for s in prepdata ]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        list_of_text=[self.int_to_str[id] for id in ids]\n",
    "        text = \" \".join(list_of_text)\n",
    "        text = re.sub(r'\\s+([,.?_!\"()\\'])',r'\\1' ,text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd091f74-2523-45bc-a72a-4a22e0e50184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd710ac5-e776-4eac-a8ce-ede923aaf9b3",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#FF5733\">\n",
    "The above 2 text are kinda our training set.  \n",
    "    \n",
    "We have separated then using the token \" <|endoftext|> \" , to mark clear seggregation.  \n",
    "    \n",
    "Also , in these 2 text , the token \"Hello\" and \"palace\" are not part of vocab , they now replace with \"<|unk|>\" token as well.  \n",
    "    \n",
    "See below example\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c22da92-96b1-4595-9ee9-a31acf1b99ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c9dc090-7c38-4911-b4a4-bd8539746b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460523f5-b2da-4162-905f-6301b742c444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
