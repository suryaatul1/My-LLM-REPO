{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa28d678-f10a-4c77-b80d-4d1d893d053c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Multi-Head attention </b> \n",
    "<p>    \n",
    "Multi Head attention is something but calculating multiple Causal Attention with different weight matrix for QUERY , KEY and VALUE \n",
    "and concatenating then together.\n",
    "\n",
    "For anology consider the statement <font color=blue> \"Atul vists Mumbai in Winters\" </font>.\n",
    "\n",
    "For this statement or input sequence to create multi attentions meaning we are trying to find answers for :\n",
    "\n",
    "<span style=color:blue>\n",
    "    \n",
    "What is happening ?? => Visit\n",
    "\n",
    "Who is visiting ?? => Atul\n",
    "\n",
    "WHen is Atul visisting => Winter\n",
    "\n",
    "</span>\n",
    "\n",
    "So we create heads (causal attention) for each of these questions and then stack/concatenate them as context rich input for pre-training\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a53dde-feab-40d1-a120-1226f9bcaa6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebddff7-3609-4a37-9109-49235466310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31167e1c-61ce-43e3-81c7-b8691d809515",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Multi-Head attention Class</b> \n",
    "<p>    \n",
    "\n",
    "Lets create the Mutli-head attention Wrapper.\n",
    "\n",
    "This wrapper calls the Causual attention for N number of time.\n",
    "\n",
    "This N is number of attention heads we want to create\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6928c6e-c289-40ab-b687-67e187d7f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image this is the Token embedding for the input sentence = \"your joruney starts with one step\"\n",
    "\n",
    "inputs = torch.tensor(\n",
    " [[0.43, 0.15, 0.89],  # Your\n",
    " [0.55, 0.87, 0.66],  # journey\n",
    " [0.57, 0.85, 0.64],  # starts\n",
    " [0.22, 0.58, 0.33],  # with\n",
    " [0.77, 0.25, 0.10],  # one\n",
    " [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1549f1e7-c5b8-499f-88b5-e86ca7b3355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have dataloader that will build the datasets in batches\n",
    "# we create a batch with 3 input , for simiplicity now\n",
    "# meaning we will have 3 sentence , with 6 tokens each , and 3 dimension , in batch pf 3\n",
    "\n",
    "batch=torch.stack((inputs,inputs,inputs),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c11b8ed-3315-446a-9a11-eca65703b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_int , d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Step 1 :- Define the query , key and valye matrices with randowm values\n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)  # Define the dropout\n",
    "        self.register_buffer('upper_mask_matrix' , torch.triu(torch.ones(context_length , context_length) , diagonal=1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # STep2 : create the key , queries and values by multiplying token embedding\n",
    "        batch , num_tokens , test = x.shape\n",
    "        keys    = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values  = self.W_value(x)\n",
    "\n",
    "        # Step 3 : Create attention scores , these are raw attention score\n",
    "        # The transpose(1,2) means just consider the data (tokens & dimensions) , as input has dimension = [ batch , token , dimension]\n",
    "        attention_scores_raw = queries @ keys.transpose(1,2)\n",
    "\n",
    "        # Step 4 :-  Implement the Causal Attention mechanism , buy masking future\n",
    "        #            Input sequences after the given token\n",
    "        attention_scores_raw.masked_fill_(\n",
    "            self.upper_mask_matrix.bool()[:num_tokens , :num_tokens], -torch.inf) # [:num_tokens ,:num_tokens] is list comprehension .\n",
    "                                                                            # If we have less tokens in one batch (E.g End of datasets with just 2 words) , to ensure it works we specify the dim  so it is broadcasted\n",
    "\n",
    "        # Step 5 :- Normalize the attention scores by diding then with sqrt od dims and softmax\n",
    "        attention_scores_norm = torch.softmax(\n",
    "              attention_scores_raw / keys.shape[-1]**0.5 , dim=-1\n",
    "            )\n",
    "\n",
    "\n",
    "        # Step 6: Add the droput funtionality  . it is regularization method to prevent overfitting    \n",
    "        attention_scores = self.dropout(attention_scores_norm)\n",
    "\n",
    "        \n",
    "        # Step 7: Create context vector\n",
    "        context_vector = attention_scores @ values\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fcfc7-6f80-4f94-814c-766be7ee8a7d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Torch nn ModuleList</b> \n",
    "<p>    \n",
    "\n",
    "This is very simple.\n",
    "\n",
    "In the __init__ method of the class , we have intialized the \"heads\" with Module=CausalAttention with its paramters.\n",
    "\n",
    "This is defined as a list with items=num_head (in this case it is 2).\n",
    "\n",
    "Then this List if called or iterated in forward method with input=x\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d15677-be7a-4633-8162-0f4d5e9584d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in , d_out , context_length , dropout , num_heads , qkv_bias=False):\n",
    "        super().__init__()\n",
    "        print(\"calling Init\")\n",
    "        self.heads = nn.ModuleList(\n",
    "                                    [CausalAttention(d_in , d_out , context_length ,dropout ,qkv_bias)\n",
    "                                     for _ in range(num_heads)]\n",
    "                                    )\n",
    "\n",
    "\n",
    "    def forward(self , x):\n",
    "       print(\"calling this\")\n",
    "       return torch.cat([head(x) for head in self.heads] , dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caed3edd-1649-425c-bfc3-45fb505ed5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling Init\n",
      "calling this\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in=batch.shape[2]\n",
    "d_out=2\n",
    "\n",
    "ca = MultiHeadAttentionWrapper(d_in , d_out , context_length , 0.0 ,2)\n",
    "context_vector = ca(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d84246a-a109-4020-ad3b-19ada6564080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c961d3-55f2-44cd-b912-0ca6fe1e83d3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Multi-Head attention Context Vector Dimension</b> \n",
    "<p>    \n",
    "There is very imporatnt dimension changes to be noticed in the context vector.\n",
    "\n",
    "We are concatenating the contect vectors from each heads along the columns (dim=1) . \n",
    "So that for each head with d_out parameter we get d_out * num_heads columns added.\n",
    "\n",
    "Say for example , in above example we have set d_out =2 (this is the dimensions of Weight matrices QUERY , KEY and VALUE)  . \n",
    "We have defined the num of heads (num_heads=2) .\n",
    "Each Causal attentioon will give us 2 dimension vector for 2 time hecne 2*2 = 4.\n",
    "\n",
    "\n",
    "If this num_head was set to 5 (num_heads=2 ) then d_out * num_heads (2*5=10)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c05849-d6a0-421b-8f5a-b2e3ee66ef93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Multi-Head attention </b> \n",
    "<p>   \n",
    "<span>\n",
    "The method shown above is not efficient.\n",
    "We are using sequential method to perform this.\n",
    "\n",
    "We will use different method tom perform this\n",
    "</span>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e273af-83de-4b92-bd15-8f861850a1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
